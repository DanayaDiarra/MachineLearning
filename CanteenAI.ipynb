{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "<a href=\"https://colab.research.google.com/github/DanayaDiarra/MachineLearning/blob/main/CanteenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>"}, {"cell_type": "markdown", "metadata": {}, "source": "# üçΩÔ∏è Canteen.ai\n## Meal Classification System for Digital Canteen\n\nThis notebook demonstrates a zero-shot meal classification pipeline using CLIP and provides an optional Stable Diffusion image-generation helper (gated)."}, {"cell_type": "code", "metadata": {}, "source": "!pip install transformers torch torchvision pillow requests diffusers accelerate safetensors", "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": "# Imports\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","execution_count": null,"outputs": []},{"cell_type": "code","metadata": {},"source": "# Load CLIP model and processor\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nmodel = model.to(device)\n\nMEAL_CLASSES = [\n    \"hamburger with fries and cola\",\n    \"pizza with salad and juice\",\n    \"chicken rice with vegetables and water\",\n    \"pasta with garlic bread and lemonade\",\n    \"sushi set with miso soup and green tea\"\n]\n\nMEAL_PRICES = {\n    \"hamburger with fries and cola\": 8.99,\n    \"pizza with salad and juice\": 10.50,\n    \"chicken rice with vegetables and water\": 7.25,\n    \"pasta with garlic bread and lemonade\": 9.75,\n    \"sushi set with miso soup and green tea\": 12.99\n}","execution_count": null,"outputs": []},{"cell_type": "code","metadata": {},"source": "def zeroshot_classification(image, classes):\n    \"""Perform zero-shot classification on a PIL image or image path.\n    Returns a dict of class -> probability.\n    \"""\n    if isinstance(image, str):\n        image = Image.open(image).convert(\"RGB\")\n    elif isinstance(image, Image.Image):\n        image = image.convert(\"RGB\")\n\n    inputs = processor(text=classes, images=image, return_tensors=\"pt\", padding=True).to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n    results = {c: p.item() for c, p in zip(classes, probs[0])}\n    return results\n\n\ndef get_meal_price(image):\n    preds = zeroshot_classification(image, MEAL_CLASSES)\n    predicted = max(preds, key=preds.get)\n    confidence = preds[predicted]\n    price = MEAL_PRICES.get(predicted, 0)\n    return predicted, confidence, price","execution_count": null,"outputs": []},{"cell_type": "code","metadata": {},"source": "def display_results(image):\n    predicted, confidence, price = get_meal_price(image)\n    print(\"üçΩÔ∏è Canteen.ai Classification Results\")\n    print(\"-\" * 40)\n    print(f\"Predicted Meal: {predicted}\")\n    print(f\"Confidence: {confidence:.2%}\")\n    print(f\"Total Price: ${price:.2f}\")\n    print(\"-\" * 40)\n    return predicted, confidence, price","execution_count": null,"outputs": []},{"cell_type": "code","metadata": {},"source": "# Example test images (replace with your own)\nsample_images = {\n    \"hamburger\": \"https://biteswithbri.com/wp-content/uploads/2021/02/HamburgerPattyRecipe04-500x375.jpg\",\n    \"pizza\": \"https://images.unsplash.com/photo-1548365328-9b4a6e8c6db2\",\n    \"chicken_rice\": \"https://images.unsplash.com/photo-1604908177522-7a2fbd0b6c2a\",\n    \"pasta\": \"https://images.unsplash.com/photo-1512058564366-c9e3c0b8a9d6\",\n    \"sushi\": \"https://images.unsplash.com/photo-1562158070-5b4f7b6f0f5a\"\n}\n\nfor name, url in sample_images.items():\n    try:\n        img = Image.open(requests.get(url, stream=True).raw)\n        print(f\"\\nTesting {name}...\")\n        display_results(img)\n    except Exception as e:\n        print(f\"Error loading {name}: {e}\")","execution_count": null,"outputs": []},{"cell_type": "code","metadata": {},"source": "# Optional: generate synthetic images with Stable Diffusion if you have HF_TOKEN and GPU\ndef generate_synthetic_test_images():\n    try:\n        from diffusers import StableDiffusionPipeline\n        import os\n        hf_token = os.environ.get(\"HF_TOKEN\")\n        if not hf_token:\n            print(\"HF_TOKEN not set; skipping image generation.\")\n            return None\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_auth_token=hf_token)\n        pipe = pipe.to(device)\n        images = {}\n        for cls in MEAL_CLASSES:\n            img = pipe(f\"professional food photography of {cls}, high quality\").images[0]\n            path = cls.replace(\" \", \"_\") + \".png\"\n            img.save(path)\n            images[cls] = img\n        return images\n    except Exception as e:\n        print(f\"Synthetic generation failed: {e}\")\n        return None","execution_count": null,"outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 5}